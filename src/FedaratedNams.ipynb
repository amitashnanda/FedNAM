{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Sequence, Tuple\n",
        "import pandas as pd\n",
        "from collections import OrderedDict, defaultdict\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_equations = []\n",
        "\n",
        "def fed_model(testimages):\n",
        "    # Load the Iris dataset\n",
        "    iris = datasets.load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    # Create TensorDataset and DataLoader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # Number of clients\n",
        "    n_clients = 3\n",
        "\n",
        "    # Split the training data into n_clients parts\n",
        "    indices = np.arange(len(train_dataset))\n",
        "    np.random.shuffle(indices)\n",
        "    split_indices = np.array_split(indices, n_clients)\n",
        "\n",
        "    # Create data loaders for each client\n",
        "    client_loaders = []\n",
        "    batch_size = 16\n",
        "    for client_indices in split_indices:\n",
        "        client_subset = Subset(train_dataset, client_indices)\n",
        "        client_loader = DataLoader(client_subset, batch_size=batch_size, shuffle=True)\n",
        "        client_loaders.append(client_loader)\n",
        "\n",
        "    # Define a simple feedforward neural network model\n",
        "    class SimpleNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(SimpleNN, self).__init__()\n",
        "            self.fc1 = nn.Linear(4, 50)  # Adjust input size to match the number of features (4 for Iris)\n",
        "            self.fc2 = nn.Linear(50, 20)\n",
        "            self.fc3 = nn.Linear(20, 3)  # Adjust output size to match the number of classes (3 for Iris)\n",
        "            self.relu = nn.ReLU()\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.relu(self.fc1(x))\n",
        "            x = self.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n",
        "\n",
        "    # Train the model on each client's data and save the weights\n",
        "    client_models = []\n",
        "    epochs = 20\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for i, loader in enumerate(client_loaders):\n",
        "        model = SimpleNN()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            # print(f'Client {i+1}, Epoch {epoch+1}, Loss: {running_loss / len(loader)}')\n",
        "\n",
        "        # Save the model weights\n",
        "        torch.save(model.state_dict(), f'client_{i+1}_model.pth')\n",
        "        client_models.append(model.state_dict())\n",
        "\n",
        "    # Federated averaging\n",
        "    sums = defaultdict(int)\n",
        "    count = len(client_models)\n",
        "    for od in client_models:\n",
        "        for key, value in od.items():\n",
        "            sums[key] += value\n",
        "\n",
        "    # Calculate the average for each key\n",
        "    averages = {key: value / count for key, value in sums.items()}\n",
        "\n",
        "    # Convert the averages to an OrderedDict (optional)\n",
        "    average_ordereddict = OrderedDict(averages)\n",
        "\n",
        "    model = SimpleNN()\n",
        "    model.load_state_dict(average_ordereddict)\n",
        "    testimages = torch.tensor(testimages, dtype=torch.float32)\n",
        "    y_test = model(testimages)\n",
        "    return y_test\n",
        "\n",
        "# Configuration class\n",
        "class Config:\n",
        "    def __init__(self, dropout=0.5, learning_rate=0.001, num_epochs=50, batch_size=32):\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "# Base Model class (assuming you have this implemented)\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, config, name):\n",
        "        super(Model, self).__init__()\n",
        "        self.config = config\n",
        "        self.name = name\n",
        "\n",
        "# FeatureNN class (assuming you have this implemented)\n",
        "class FeatureNN(nn.Module):\n",
        "    def __init__(self, config, name, input_shape, num_units, feature_num):\n",
        "        super(FeatureNN, self).__init__()\n",
        "        self.config = config\n",
        "        self.name = name\n",
        "        self.input_shape = input_shape\n",
        "        self.num_units = num_units\n",
        "        self.feature_num = feature_num\n",
        "        self.fc = nn.Linear(input_shape, num_units)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "# NAM model definition\n",
        "class NAM(Model):\n",
        "    def __init__(self, config, name, *, num_inputs: int, num_units: int) -> None:\n",
        "        super(NAM, self).__init__(config, name)\n",
        "        self._num_inputs = num_inputs\n",
        "        self.dropout = nn.Dropout(p=self.config.dropout)\n",
        "\n",
        "        if isinstance(num_units, list):\n",
        "            assert len(num_units) == num_inputs\n",
        "            self._num_units = num_units\n",
        "        elif isinstance(num_units, int):\n",
        "            self._num_units = [num_units for _ in range(self._num_inputs)]\n",
        "\n",
        "        self.feature_nns = nn.ModuleList([\n",
        "            FeatureNN(config=config, name=f'FeatureNN_{i}', input_shape=1, num_units=self._num_units[i], feature_num=i)\n",
        "            for i in range(num_inputs)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(sum(self._num_units), 3)  # 3 classes for Iris dataset\n",
        "        self._bias = torch.nn.Parameter(data=torch.zeros(1))\n",
        "\n",
        "    def calc_outputs(self, inputs: torch.Tensor) -> Sequence[torch.Tensor]:\n",
        "        return [self.feature_nns[i](inputs[:, i:i+1]) for i in range(self._num_inputs)]\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        individual_outputs = self.calc_outputs(inputs)\n",
        "        conc_out = torch.cat(individual_outputs, dim=-1)\n",
        "        dropout_out = self.dropout(conc_out)\n",
        "        out = self.output_layer(dropout_out)\n",
        "        return out, dropout_out\n",
        "\n",
        "    def print_model_equation(self, feature_names):\n",
        "        equation_terms = []\n",
        "        feature_contributions = {}\n",
        "        for i, fnn in enumerate(self.feature_nns):\n",
        "            coefficients = fnn.fc.weight.data.flatten().tolist()\n",
        "            intercepts = fnn.fc.bias.data.tolist()\n",
        "            term = \" + \".join([f\"({coeff:.3f} * x_{feature_names[i]} + {intercept:.3f})\" for coeff, intercept in zip(coefficients, intercepts)])\n",
        "            equation_terms.append(term)\n",
        "            feature_contributions[feature_names[i]] = sum(abs(c) for c in coefficients)\n",
        "        equation = \" + \".join(equation_terms) + f\" + bias ({self._bias.item():.3f})\"\n",
        "        print(f\"Model Equation: y = {equation}\")\n",
        "        model_equations.append(equation)\n",
        "\n",
        "        # Determine feature interpretability based on coefficients\n",
        "        interpretability = sorted(feature_contributions.items(), key=lambda x: x[1], reverse=True)\n",
        "        print(\"\\nFeature Contributions:\")\n",
        "        for feature, contribution in interpretability:\n",
        "            print(f\"{feature}: {contribution:.3f}\")\n",
        "\n",
        "        return interpretability[0][0]  # Return the feature with the highest contribution\n",
        "\n",
        "n_clients = 3\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_columns = iris.feature_names\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into n_clients\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "split_indices = np.array_split(indices, n_clients)\n",
        "client_data = [(X[indices], y[indices]) for indices in split_indices]\n",
        "\n",
        "for i in range(n_clients):\n",
        "    X, y = client_data[i]\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    # Define the config\n",
        "    config = Config(dropout=0.5, learning_rate=0.001, num_epochs=50, batch_size=32)\n",
        "\n",
        "    # Instantiate the NAM model\n",
        "    num_inputs = len(feature_columns)  # Number of features\n",
        "    num_units = 10  # Number of units in the hidden layer\n",
        "    nam_model = NAM(config=config, name='NAM_Model', num_inputs=num_inputs, num_units=num_units)\n",
        "\n",
        "    # Training function\n",
        "    def train(model, X_train, y_train, config):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "        model.train()\n",
        "        for epoch in range(config.num_epochs):\n",
        "            outputs = fed_model(X_test_tensor)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(outputs, y_test_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f'Epoch [{epoch + 1}/{config.num_epochs}], Loss: {loss.item():.4f}')\n",
        "        return model\n",
        "\n",
        "    # Evaluation function\n",
        "    def evaluate(model, X_test, y_test):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs, _ = model(X_test_tensor)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
        "            print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train(nam_model, X_train_tensor, y_train_tensor, config)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate(trained_model, X_test_tensor, y_test_tensor)\n",
        "\n",
        "    # Print the model equation and get the most contributing feature\n",
        "    most_contributing_feature = trained_model.print_model_equation(feature_columns)\n",
        "    print(f\"\\nMost contributing feature for client's output {i}: {most_contributing_feature}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt_SRgr5LYqc",
        "outputId": "001861d6-e53c-4ce2-d04c-8a56a153d96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Loss: 0.9898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/50], Loss: 0.9417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/50], Loss: 1.0564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/50], Loss: 0.8475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/50], Loss: 1.0457\n",
            "Accuracy: 26.67%\n",
            "Model Equation: y = (-0.357 * x_sepal length (cm) + 0.971) + (-0.778 * x_sepal length (cm) + 0.007) + (0.519 * x_sepal length (cm) + 0.852) + (0.661 * x_sepal length (cm) + -0.670) + (-0.432 * x_sepal length (cm) + 0.580) + (-0.022 * x_sepal length (cm) + 0.341) + (0.456 * x_sepal length (cm) + 0.956) + (-0.868 * x_sepal length (cm) + -0.559) + (-0.107 * x_sepal length (cm) + -0.425) + (0.010 * x_sepal length (cm) + 0.657) + (-0.377 * x_sepal width (cm) + -0.399) + (-0.615 * x_sepal width (cm) + 0.452) + (0.985 * x_sepal width (cm) + -0.292) + (0.249 * x_sepal width (cm) + 0.726) + (-0.145 * x_sepal width (cm) + -0.657) + (-0.290 * x_sepal width (cm) + 0.167) + (-0.879 * x_sepal width (cm) + 0.642) + (-0.559 * x_sepal width (cm) + -0.269) + (0.643 * x_sepal width (cm) + 0.735) + (0.013 * x_sepal width (cm) + 0.303) + (0.861 * x_petal length (cm) + 0.502) + (0.466 * x_petal length (cm) + -0.854) + (0.697 * x_petal length (cm) + 0.162) + (0.437 * x_petal length (cm) + 0.709) + (-0.464 * x_petal length (cm) + -0.314) + (0.208 * x_petal length (cm) + 0.217) + (-0.974 * x_petal length (cm) + -0.857) + (-0.864 * x_petal length (cm) + 0.627) + (0.537 * x_petal length (cm) + -0.100) + (0.588 * x_petal length (cm) + -0.030) + (-0.448 * x_petal width (cm) + 0.632) + (0.438 * x_petal width (cm) + 0.283) + (0.434 * x_petal width (cm) + 0.331) + (0.137 * x_petal width (cm) + -0.778) + (0.151 * x_petal width (cm) + 0.373) + (-0.399 * x_petal width (cm) + 0.940) + (0.561 * x_petal width (cm) + -0.707) + (0.569 * x_petal width (cm) + 0.715) + (0.228 * x_petal width (cm) + -0.425) + (-0.853 * x_petal width (cm) + 0.467) + bias (0.000)\n",
            "\n",
            "Feature Contributions:\n",
            "petal length (cm): 6.095\n",
            "sepal width (cm): 4.756\n",
            "petal width (cm): 4.218\n",
            "sepal length (cm): 4.209\n",
            "\n",
            "Most contributing feature for client's output 0: petal length (cm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Loss: 0.9156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/50], Loss: 0.9606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/50], Loss: 0.9803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/50], Loss: 1.0143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/50], Loss: 0.9843\n",
            "Accuracy: 26.67%\n",
            "Model Equation: y = (0.379 * x_sepal length (cm) + 0.686) + (-0.387 * x_sepal length (cm) + -0.554) + (0.728 * x_sepal length (cm) + 0.455) + (-0.480 * x_sepal length (cm) + -0.706) + (0.673 * x_sepal length (cm) + 0.145) + (-0.348 * x_sepal length (cm) + -0.659) + (0.105 * x_sepal length (cm) + 0.500) + (0.090 * x_sepal length (cm) + 0.269) + (0.287 * x_sepal length (cm) + 0.576) + (-0.419 * x_sepal length (cm) + -0.608) + (0.961 * x_sepal width (cm) + -0.450) + (0.592 * x_sepal width (cm) + -0.219) + (0.073 * x_sepal width (cm) + -0.689) + (0.367 * x_sepal width (cm) + 0.776) + (-0.428 * x_sepal width (cm) + -0.851) + (-0.797 * x_sepal width (cm) + -0.281) + (0.333 * x_sepal width (cm) + 0.823) + (-0.657 * x_sepal width (cm) + 0.846) + (-0.363 * x_sepal width (cm) + -0.716) + (-0.585 * x_sepal width (cm) + -0.356) + (-0.045 * x_petal length (cm) + -0.050) + (0.077 * x_petal length (cm) + -0.362) + (0.816 * x_petal length (cm) + -0.846) + (-0.371 * x_petal length (cm) + -0.432) + (-0.652 * x_petal length (cm) + -0.259) + (0.562 * x_petal length (cm) + 0.318) + (-0.891 * x_petal length (cm) + 0.512) + (0.651 * x_petal length (cm) + 0.476) + (0.226 * x_petal length (cm) + -0.846) + (0.746 * x_petal length (cm) + -0.517) + (0.497 * x_petal width (cm) + 0.375) + (-0.384 * x_petal width (cm) + 0.929) + (-0.838 * x_petal width (cm) + -0.438) + (0.629 * x_petal width (cm) + 0.346) + (-0.924 * x_petal width (cm) + -0.039) + (0.199 * x_petal width (cm) + -0.459) + (0.988 * x_petal width (cm) + 0.400) + (-0.690 * x_petal width (cm) + -0.240) + (-0.812 * x_petal width (cm) + 0.810) + (0.363 * x_petal width (cm) + -0.683) + bias (0.000)\n",
            "\n",
            "Feature Contributions:\n",
            "petal width (cm): 6.325\n",
            "sepal width (cm): 5.156\n",
            "petal length (cm): 5.039\n",
            "sepal length (cm): 3.896\n",
            "\n",
            "Most contributing feature for client's output 1: petal width (cm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Loss: 0.8763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/50], Loss: 0.8842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/50], Loss: 0.9541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/50], Loss: 0.7706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n",
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/50], Loss: 0.8331\n",
            "Accuracy: 33.33%\n",
            "Model Equation: y = (0.664 * x_sepal length (cm) + -0.752) + (0.276 * x_sepal length (cm) + -0.238) + (-0.852 * x_sepal length (cm) + -0.019) + (0.570 * x_sepal length (cm) + 0.910) + (0.112 * x_sepal length (cm) + -0.555) + (-0.098 * x_sepal length (cm) + 0.702) + (-0.506 * x_sepal length (cm) + 0.895) + (-0.568 * x_sepal length (cm) + -0.537) + (-0.291 * x_sepal length (cm) + 0.135) + (-0.233 * x_sepal length (cm) + 0.329) + (-0.240 * x_sepal width (cm) + -0.795) + (0.736 * x_sepal width (cm) + 0.197) + (0.087 * x_sepal width (cm) + 0.684) + (-0.140 * x_sepal width (cm) + 0.729) + (-0.751 * x_sepal width (cm) + -0.232) + (-0.256 * x_sepal width (cm) + 0.162) + (0.565 * x_sepal width (cm) + -0.868) + (0.504 * x_sepal width (cm) + -0.151) + (-0.364 * x_sepal width (cm) + 0.818) + (0.842 * x_sepal width (cm) + 0.136) + (-0.181 * x_petal length (cm) + -0.623) + (0.346 * x_petal length (cm) + 0.310) + (0.921 * x_petal length (cm) + 0.640) + (-0.579 * x_petal length (cm) + -0.165) + (-0.739 * x_petal length (cm) + 0.713) + (-0.216 * x_petal length (cm) + 0.723) + (-0.465 * x_petal length (cm) + -0.086) + (0.852 * x_petal length (cm) + -0.203) + (-0.189 * x_petal length (cm) + -0.374) + (0.058 * x_petal length (cm) + 0.069) + (0.839 * x_petal width (cm) + -0.685) + (-0.375 * x_petal width (cm) + -0.317) + (0.741 * x_petal width (cm) + -0.323) + (0.070 * x_petal width (cm) + -0.751) + (0.684 * x_petal width (cm) + 0.567) + (0.602 * x_petal width (cm) + -0.284) + (-0.397 * x_petal width (cm) + 0.730) + (0.590 * x_petal width (cm) + 0.472) + (0.348 * x_petal width (cm) + 0.680) + (-0.923 * x_petal width (cm) + -0.770) + bias (0.000)\n",
            "\n",
            "Feature Contributions:\n",
            "petal width (cm): 5.568\n",
            "petal length (cm): 4.544\n",
            "sepal width (cm): 4.485\n",
            "sepal length (cm): 4.171\n",
            "\n",
            "Most contributing feature for client's output 2: petal width (cm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e162dc89bfec>:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testimages = torch.tensor(testimages, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "81whVmhg97sd",
        "outputId": "b5401eae-ac5c-4d31-87ee-a240702fdaa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43103711.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bb5920bcd9f4>\u001b[0m in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# Get the top 30% contributing pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mtop_pixels_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_contributing_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mhighlighted_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Convert the mask to coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Define the transformation for the CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define a simple CNN model for image classification\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Train the model on each client's data and save the weights\n",
        "epochs = 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, trainloader, epochs, criterion):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = SimpleCNN()\n",
        "trained_model = train_model(model, trainloader, epochs, criterion)\n",
        "\n",
        "# Function to get the top 30% contributing pixels\n",
        "def get_top_contributing_pixels(model, data, percentage=0.3):\n",
        "    model.eval()\n",
        "    data.requires_grad_()\n",
        "    output = model(data)\n",
        "    pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "    model.zero_grad()\n",
        "    output[0, pred_class].backward()\n",
        "    grad = data.grad.abs().squeeze().detach().numpy()\n",
        "\n",
        "    # Flatten the gradient array and get the threshold value\n",
        "    flattened_grad = grad.flatten()\n",
        "    threshold = np.percentile(flattened_grad, 100 - percentage * 100)\n",
        "\n",
        "    # Create a mask of the top contributing pixels\n",
        "    top_pixels_mask = grad >= threshold\n",
        "\n",
        "    return top_pixels_mask\n",
        "\n",
        "# Get a test image\n",
        "test_images, _ = next(iter(testloader))\n",
        "test_image = test_images[0].unsqueeze(0)\n",
        "\n",
        "# Get the top 30% contributing pixels\n",
        "top_pixels_mask = get_top_contributing_pixels(trained_model, test_image)\n",
        "highlighted_image = test_image.squeeze().numpy()\n",
        "\n",
        "# Convert the mask to coordinates\n",
        "mask_2d = top_pixels_mask\n",
        "highlighted_image[mask_2d == 0] = 0  # Mask non-top pixels\n",
        "\n",
        "# Plot the original and highlighted image\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(np.transpose(test_image.squeeze().numpy(), (1, 2, 0)))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Highlighted Image\")\n",
        "plt.imshow(np.transpose(highlighted_image, (1, 2, 0)))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Configuration class\n",
        "class Config:\n",
        "    def __init__(self, dropout=0.5, learning_rate=0.001, num_epochs=50, batch_size=32):\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "# Base Model class (assuming you have this implemented)\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, config, name):\n",
        "        super(Model, self).__init__()\n",
        "        self.config = config\n",
        "        self.name = name\n",
        "\n",
        "# FeatureNN class (assuming you have this implemented)\n",
        "class FeatureNN(nn.Module):\n",
        "    def __init__(self, config, name, input_shape, num_units, feature_num):\n",
        "        super(FeatureNN, self).__init__()\n",
        "        self.config = config\n",
        "        self.name = name\n",
        "        self.input_shape = input_shape\n",
        "        self.num_units = num_units\n",
        "        self.feature_num = feature_num\n",
        "        self.fc = nn.Linear(input_shape, num_units)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "# NAM model definition\n",
        "class NAM(Model):\n",
        "    def __init__(self, config, name, *, num_inputs: int, num_units: int) -> None:\n",
        "        super(NAM, self).__init__(config, name)\n",
        "        self._num_inputs = num_inputs\n",
        "        self.dropout = nn.Dropout(p=self.config.dropout)\n",
        "\n",
        "        if isinstance(num_units, list):\n",
        "            assert len(num_units) == num_inputs\n",
        "            self._num_units = num_units\n",
        "        elif isinstance(num_units, int):\n",
        "            self._num_units = [num_units for _ in range(self._num_inputs)]\n",
        "\n",
        "        self.feature_nns = nn.ModuleList([\n",
        "            FeatureNN(config=config, name=f'FeatureNN_{i}', input_shape=1, num_units=self._num_units[i], feature_num=i)\n",
        "            for i in range(num_inputs)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(sum(self._num_units), 10)  # 10 classes for CIFAR-10 dataset\n",
        "        self._bias = torch.nn.Parameter(data=torch.zeros(1))\n",
        "\n",
        "    def calc_outputs(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        return [self.feature_nns[i](inputs[:, i:i+1]) for i in range(self._num_inputs)]\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        individual_outputs = self.calc_outputs(inputs)\n",
        "        conc_out = torch.cat(individual_outputs, dim=-1)\n",
        "        dropout_out = self.dropout(conc_out)\n",
        "        out = self.output_layer(dropout_out)\n",
        "        return out, dropout_out\n",
        "\n",
        "    def print_model_equation(self, feature_names):\n",
        "        equation_terms = []\n",
        "        feature_contributions = {}\n",
        "        for i, fnn in enumerate(self.feature_nns):\n",
        "            coefficients = fnn.fc.weight.data.flatten().tolist()\n",
        "            intercepts = fnn.fc.bias.data.tolist()\n",
        "            term = \" + \".join([f\"({coeff:.3f} * x_{feature_names[i]} + {intercept:.3f})\" for coeff, intercept in zip(coefficients, intercepts)])\n",
        "            equation_terms.append(term)\n",
        "            feature_contributions[feature_names[i]] = sum(abs(c) for c in coefficients)\n",
        "        equation = \" + \".join(equation_terms) + f\" + bias ({self._bias.item():.3f})\"\n",
        "        print(f\"Model Equation: y = {equation}\")\n",
        "\n",
        "        # Determine feature interpretability based on coefficients\n",
        "        interpretability = sorted(feature_contributions.items(), key=lambda x: x[1], reverse=True)\n",
        "        print(\"\\nFeature Contributions:\")\n",
        "        for feature, contribution in interpretability:\n",
        "            print(f\"{feature}: {contribution:.3f}\")\n",
        "\n",
        "        return interpretability[0][0]  # Return the feature with the highest contribution\n",
        "\n",
        "# Define the config\n",
        "config = Config(dropout=0.5, learning_rate=0.001, num_epochs=50, batch_size=32)\n",
        "\n",
        "# Instantiate the NAM model\n",
        "num_inputs = 32 * 32 * 3  # Number of features for CIFAR-10 images (32x32 and 3 channels)\n",
        "num_units = 10  # Number of units in the hidden layer\n",
        "nam_model = NAM(config=config, name='NAM_Model', num_inputs=num_inputs, num_units=num_units)\n",
        "\n",
        "# Training function for NAM model\n",
        "def train_nam_model(model, trainloader, config):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "    model.train()\n",
        "    for epoch in range(config.num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs.view(inputs.size(0), -1))  # Flatten the inputs\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{config.num_epochs}], Loss: {running_loss / len(trainloader):.4f}')\n",
        "    return model\n",
        "\n",
        "# Train the NAM model\n",
        "trained_nam_model = train_nam_model(nam_model, trainloader, config)\n",
        "\n",
        "# Evaluate the NAM model\n",
        "def evaluate_nam_model(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            outputs, _ = model(inputs.view(inputs.size(0), -1))  # Flatten the inputs\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "evaluate_nam_model(trained_nam_model, testloader)\n",
        "\n",
        "# Print the model equation and get the most contributing feature\n",
        "feature_names = [f'pixel_{i}' for i in range(num_inputs)]\n",
        "most_contributing_feature = trained_nam_model.print_model_equation(feature_names)\n",
        "print(f\"\\nMost contributing feature: {most_contributing_feature}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5AjvMEC97mc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgwG1Dyf97jm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nR6jEAj97gP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R-CbyJu03CYx",
        "outputId": "1ce040af-243a-4973-e900-f9f8550c5092"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-03b2527f5eac>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Get the top 30% contributing pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mtop_pixels_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_contributing_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mhighlighted_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# tensor.detach().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Convert the mask to coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "# Define the transformation for the image dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define a simple CNN model for image classification\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Train the model on each client's data and save the weights\n",
        "epochs = 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, trainloader, epochs, criterion):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = SimpleCNN()\n",
        "trained_model = train_model(model, trainloader, epochs, criterion)\n",
        "\n",
        "# Function to get the top 30% contributing pixels\n",
        "def get_top_contributing_pixels(model, data, percentage=0.3):\n",
        "    model.eval()\n",
        "    data.requires_grad_()\n",
        "    output = model(data)\n",
        "    pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "    model.zero_grad()\n",
        "    output[0, pred_class].backward()\n",
        "    grad = data.grad.abs().squeeze().detach().numpy()\n",
        "\n",
        "    # Flatten the gradient array and get the threshold value\n",
        "    flattened_grad = grad.flatten()\n",
        "    threshold = np.percentile(flattened_grad, 100 - percentage * 100)\n",
        "\n",
        "    # Create a mask of the top contributing pixels\n",
        "    top_pixels_mask = grad >= threshold\n",
        "\n",
        "    return top_pixels_mask\n",
        "\n",
        "# Get a test image\n",
        "test_images, _ = next(iter(testloader))\n",
        "test_image = test_images[0].unsqueeze(0)\n",
        "\n",
        "# Get the top 30% contributing pixels\n",
        "top_pixels_mask = get_top_contributing_pixels(trained_model, test_image)\n",
        "highlighted_image = test_image.squeeze().numpy()  # tensor.detach().numpy()\n",
        "\n",
        "# Convert the mask to coordinates\n",
        "mask_2d = top_pixels_mask\n",
        "highlighted_image[mask_2d == 0] = 0  # Mask non-top pixels\n",
        "\n",
        "# Plot the original and highlighted image\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(test_image.squeeze().numpy(), cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Highlighted Image\")\n",
        "plt.imshow(highlighted_image, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Configuration class\n",
        "class Config:\n",
        "    def __init__(self, dropout=0.5, learning_rate=0.001, num_epochs=50, batch_size=32):\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "# Base Model class (assuming you have this implemented)\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, config, name):\n",
        "        super(Model, self).__init__()\n",
        "        self.config = config\n",
        "        self.name = name\n",
        "\n",
        "# FeatureNN class (assuming you have this implemented)\n",
        "class FeatureNN(nn.Module):\n",
        "    def __init__(self, config, name, input_shape, num_units, feature_num):\n",
        "        super(FeatureNN, self).__init__()\n",
        "        self.config = config\n",
        "        self.name = name\n",
        "        self.input_shape = input_shape\n",
        "        self.num_units = num_units\n",
        "        self.feature_num = feature_num\n",
        "        self.fc = nn.Linear(input_shape, num_units)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "# NAM model definition\n",
        "class NAM(Model):\n",
        "    def __init__(self, config, name, *, num_inputs: int, num_units: int) -> None:\n",
        "        super(NAM, self).__init__(config, name)\n",
        "        self._num_inputs = num_inputs\n",
        "        self.dropout = nn.Dropout(p=self.config.dropout)\n",
        "\n",
        "        if isinstance(num_units, list):\n",
        "            assert len(num_units) == num_inputs\n",
        "            self._num_units = num_units\n",
        "        elif isinstance(num_units, int):\n",
        "            self._num_units = [num_units for _ in range(self._num_inputs)]\n",
        "\n",
        "        self.feature_nns = nn.ModuleList([\n",
        "            FeatureNN(config=config, name=f'FeatureNN_{i}', input_shape=1, num_units=self._num_units[i], feature_num=i)\n",
        "            for i in range(num_inputs)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(sum(self._num_units), 10)  # 10 classes for MNIST dataset\n",
        "        self._bias = torch.nn.Parameter(data=torch.zeros(1))\n",
        "\n",
        "    def calc_outputs(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        return [self.feature_nns[i](inputs[:, i:i+1]) for i in range(self._num_inputs)]\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        individual_outputs = self.calc_outputs(inputs)\n",
        "        conc_out = torch.cat(individual_outputs, dim=-1)\n",
        "        dropout_out = self.dropout(conc_out)\n",
        "        out = self.output_layer(dropout_out)\n",
        "        return out, dropout_out\n",
        "\n",
        "    def print_model_equation(self, feature_names):\n",
        "        equation_terms = []\n",
        "        feature_contributions = {}\n",
        "        for i, fnn in enumerate(self.feature_nns):\n",
        "            coefficients = fnn.fc.weight.data.flatten().tolist()\n",
        "            intercepts = fnn.fc.bias.data.tolist()\n",
        "            term = \" + \".join([f\"({coeff:.3f} * x_{feature_names[i]} + {intercept:.3f})\" for coeff, intercept in zip(coefficients, intercepts)])\n",
        "            equation_terms.append(term)\n",
        "            feature_contributions[feature_names[i]] = sum(abs(c) for c in coefficients)\n",
        "        equation = \" + \".join(equation_terms) + f\" + bias ({self._bias.item():.3f})\"\n",
        "        print(f\"Model Equation: y = {equation}\")\n",
        "\n",
        "        # Determine feature interpretability based on coefficients\n",
        "        interpretability = sorted(feature_contributions.items(), key=lambda x: x[1], reverse=True)\n",
        "        print(\"\\nFeature Contributions:\")\n",
        "        for feature, contribution in interpretability:\n",
        "            print(f\"{feature}: {contribution:.3f}\")\n",
        "\n",
        "        return interpretability[0][0]  # Return the feature with the highest contribution\n",
        "\n",
        "# Define the config\n",
        "config = Config(dropout=0.5, learning_rate=0.001, num_epochs=50, batch_size=32)\n",
        "\n",
        "# Instantiate the NAM model\n",
        "num_inputs = 28 * 28  # Number of features for MNIST images\n",
        "num_units = 10  # Number of units in the hidden layer\n",
        "nam_model = NAM(config=config, name='NAM_Model', num_inputs=num_inputs, num_units=num_units)\n",
        "\n",
        "# Training function for NAM model\n",
        "def train_nam_model(model, trainloader, config):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "    model.train()\n",
        "    for epoch in range(config.num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs.view(inputs.size(0), -1))  # Flatten the inputs\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{config.num_epochs}], Loss: {running_loss / len(trainloader):.4f}')\n",
        "    return model\n",
        "\n",
        "# Train the NAM model\n",
        "trained_nam_model = train_nam_model(nam_model, trainloader, config)\n",
        "\n",
        "# Evaluate the NAM model\n",
        "def evaluate_nam_model(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            outputs, _ = model(inputs.view(inputs.size(0), -1))  # Flatten the inputs\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "evaluate_nam_model(trained_nam_model, testloader)\n",
        "\n",
        "# Print the model equation and get the most contributing feature\n",
        "feature_names = [f'pixel_{i}' for i in range(num_inputs)]\n",
        "most_contributing_feature = trained_nam_model.print_model_equation(feature_names)\n",
        "print(f\"\\nMost contributing feature: {most_contributing_feature}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}